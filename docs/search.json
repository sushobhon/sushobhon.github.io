[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLinear Regression Representated in Neural Net\n\n\n\n\n\n\nDeep Learning\n\n\nNeural Network\n\n\n\nHow one or multiple linear regression can be represented in Neural Net\n\n\n\n\n\nJul 25, 2025\n\n\nSushobhon Karmakar\n\n\n\n\n\n\n\n\n\n\n\n\nThe Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling\n\n\n\n\n\n\nGen AI\n\n\nLLM\n\n\n\nMethods LLM models used to select next token based on some given information\n\n\n\n\n\nJul 1, 2025\n\n\nSushobhon Karmakar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "Have you ever wondered how your phone magically suggests the next word as you type? Or how chatbots string together coherent sentences? The secret lies in something called next token generation.\nThink of it this way: imagine telling a story one word at a time. After saying “The big,” what might come next? Perhaps “dog,” “house,” or “tree.” Next token generation teaches computers to do exactly this — predict the most likely word (or sometimes a piece of a word, called a “token”) that should follow the current sequence.\nFor instance, if you type “Thank you for your,” a next token generation model might suggest “help” as the most likely next word. It’s like having a super-smart autocomplete feature!\nIn this blog, we’ll pull back the curtain to explore the fascinating techniques behind this technology. We’ll dive into some code to see how it works and compare our results with the leading transformer libraries.\nGet ready to explore the world of language prediction, where we’ll examine five popular methods that make next token generation possible: Greedy Search, Beam Search, Top-k Sampling, Top-p (Nucleus) Sampling, and Temperature Control. By the end, you’ll understand how machines learn to speak our language!\nIn this blog, we’ll use the gpt2-medium model to predict token probabilities, though you’re welcome to experiment with other models.\n\n\nWhen we pass text to a language model and ask it to predict the next word, it calculates the probability of each possible token given the input. The most natural and intuitive approach is to choose the token with the highest probability. This method of choosing the next token is called Greedy Search.\nIn probabilistic terms, the t th token, given the tokens from 1 to t-1, will be \nP\\big(W_t|W_{1:t-1}\\big) = \\argmax_{i} \\bigg[P\\big(W_i|W_{1:t-1}\\big)\\bigg]\n Let’s see how to implement this. First install torch, transformers, and hf_xet.\n!pip install torch transformers hf_xet\nNext, let’s import the required libraries, set up our device, and download the model.\nNow let’s define a custom function that predicts the next token using greedy search. In this function, we first predict the logits score for each token, then convert those to probabilities using the softmax function.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps):\n    \"\"\"\n    Predict next word using Greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n\n        # Returning iterations as df\n        return tokenizer.decode(input_ids[0])\n\nLet us pass i love and predict next few word using our custom function and compare the result with transformers library.\n\ninput_text = \"i love\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\ngenerated_text = greedy_search(input_text=input_text,\n                              input_ids=input_ids,\n                              time_steps=20     # Generating Next 20 tokens\n                              )\nprint(generated_text)\n\nThe output of our custom greedy_search() is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nLet’s predicted tokens using transformers library.\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=20)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\n\nOutput of transformer library is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nGenerated words are exactly same hah!!\nSince this is an deterministic approach predicted words are exactly same.\nIf you want to find out what are the other probable tokens and what is there probability you can try this code.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps, choices_per_step):\n    \"\"\"\n    Predict next word using greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n        choices_per_step (int): Number of choice at each step.\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n\n            # Top few highest tokens\n            for choics_idx in range(choices_per_step):\n                token_index_sorted = sorted_index_of_next_probability[choics_idx]\n                token_prob = next_token_probability[token_index_sorted].cpu().numpy()\n                token_choice = f\"{tokenizer.decode(token_index_sorted)} ({token_prob*100:.2f}%)\"\n                iteration[f'Choice {choics_idx+1}'] = token_choice\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n            iterations.append(iteration)\n\n        # Returning iterations as df\n        return pd.DataFrame(iterations)\n\n\n\n\nOne of the main disadvantage of Greedy Search is, it fails to find out high probable words hidden behind low probability word. To fix this, there’s a smarter method called Beam Search. Beam Search doesn’t just pick one word at a time—it keeps track of a few possible paths and explores them to find the best combination of words.\nLet’s see how it works with an easy example:\n\n\n\nfig-1: Beam Search Example\n\n\nIn the preceding example, we’ve considered the initial tokens i and love The model predicts five potential next tokens, with the exhibiting the highest probability. Consequently, a Greedy search strategy would select the as the subsequent token, followed by the token with the highest probability given the, which in this case is idea.\nBut in Beam search for 2 beams we will be calculating conditional probability of next 2 tokens. The conditional probabilities are:\n\nP('the','way'|'i','love') = 0.25 \\times 0.03 = 0.0075\n \nP('the','idea'|'i','love')=0.25\\times 0.45 = 0.1125\n \nP('the','look'|'i','love')=0.25\\times0.25=0.0625\n ...\n\n\nP('it','.'|'i','love')=0.23\\times0.6=0.138\n\n\n\nP('it','mom'|'i','love')=0.23\\times0.4=0.092\n We can observe, the conditional probability of the sequence it and . is the highest among the considered pairs. Therefore, in Beam Search with a beam width of 2, it will be selected as the first subsequent token, rather than the, despite the having the highest individual probability in the previous step.\nLet’s try to write a function for beam search\n\n# Beam search function\ndef beam_search(input_text, max_length=50, num_beams=5):\n    \"\"\"\n    Predict next word using Beam Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        num_beams (int): Number of Beams to consider.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    # Initialize beams: Each beam starts with the same initial input\n    beams = [(input_ids, 0)]  # Tuple of (tokens, score)\n    \n    for _ in range(max_length):\n        new_beams = []\n        \n        for tokens, score in beams:\n            # Get model predictions\n            with torch.no_grad():\n                outputs = model(tokens)\n            \n            # Extract logits for the last token and apply softmax\n            logits = outputs.logits[:, -1, :]\n            probs = torch.softmax(logits, dim=-1)\n            \n            # Get top beam candidates\n            top_probs, top_indices = probs.topk(num_beams)\n            \n            # Create new beams\n            for i in range(num_beams):\n                new_token = top_indices[:, i].unsqueeze(-1)\n                new_score = score + torch.log(top_probs[:, i])  # Update score\n                \n                new_beam = (torch.cat([tokens, new_token], dim=-1), new_score)\n                new_beams.append(new_beam)\n        \n        # Sort beams by score and keep the best ones\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:num_beams]\n    \n    # Select the best final beam\n    best_tokens = beams[0][0]\n    return tokenizer.decode(best_tokens[0], skip_special_tokens=True)\n\nLet’s Try with the same input i love and then try to generate next 2 tokens using beam_search() function:\n# Example usage\ninput_text = \"i love\"\ngenerated_text = beam_search(input_text, max_length=10, num_beams=2)  # num_beams = 1 is same as greedy search\nprint(generated_text)\ni love it. I love it. I love it.\nOne thing to note, Beam Search with one beam is same as greedy search.(Verify!)\nNow Let’s us compare the result form our function and output from transformers library.\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=10, num_beams = 3)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\ni love it. I love it. I love it.\nInterestingly, the outputs in both scenarios end up being identical. However, a noticeable issue is the repetitiveness of the generated text. This tendency towards repetition is a common drawback of deterministic decoding strategies. To mitigate this, we can introduce an element of randomness into the token selection process.\n\n\n\nTo introduce randomness into the token selection process, one effective method is to randomly choose a token from the top k most probable predictions. This technique is known as Top-k Sampling or Top-k Search. The process involves the following steps:\n\nPredict Probabilities: First, the model predicts the probability distribution over all possible tokens.\nSelect Top-k: We then identify and select the k tokens with the highest probabilities.\nNormalize Probabilities: The probabilities of these k selected tokens are re-normalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_k_search(input_text, max_length=50, k=5, show_option= False):\n        \"\"\"\n    Predict next word using top k Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        k (int): Number of high probability token to consider.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      top_probs, top_indices = probs.topk(k)\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=1, k=5, show_option = True)  # num_beams = 1 is same as gready search\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTop 5 most probable tokens are:\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (22.54%)\n it (22.47%)\n you (21.72%)\n to (19.86%)\n this (13.4%)\nThe next selected token is the. At this step, the five most probable predicted tokens are the, it, you, to, and this. From these five, one token will be randomly selected based on their respective probabilities.\nThe subsequent words were generated using the transformers library.\n# set seed to reproduce results. Feel free to change the seed though to get different results\nfrom transformers import set_seed\n\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# set top_k to 50\nsample_output = model.generate(\n    **input_tokens,\n    max_new_tokens=4,\n    do_sample=True,\n    top_k=5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))\ni love the \nIn this instance, the predicted token is also the, although this is purely coincidental. Rerunning the code might yield a different token, but it’s important to note that the subsequent selected token will invariably be one of the five most probable tokens from the preceding output (as you can verify!).\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=10, k=5)\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love the way he's doing this, but I'm\n--------------------\nIf we predict next 10 tokens we can see there is not any repetition to tokens like previous case.\n\n\n\nAnother effective method involves considering a dynamic set of the most probable tokens whose cumulative probability exceeds a predefined threshold, p. This technique is known as Top-p Sampling or Nucleus Sampling.\nThe process unfolds as follows:\n\nPredict Probabilities: Initially, the model predicts the probability distribution over all possible tokens.\nIdentify the Nucleus: We then identify the smallest set of most probable tokens such that the sum of their probabilities is greater than or equal to the probability threshold p.\nNormalize Probabilities: The probabilities of the tokens within this selected set (the “nucleus”) are renormalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_p_search(input_text, max_length=50, p=1, show_option= False):\n        \"\"\"\n    Predict next word using top p Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=1, p=0.5, show_option= True)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (17.06%)\n it (17.01%)\n you (16.44%)\n to (15.03%)\n this (10.14%)\n that (7.91%)\n my (5.68%)\n her (3.64%)\n him (3.56%)\n them (3.53%)\n--------------------\nFinal generated Text is:\ni love the\n--------------------\nwith p=0.5 when we predict next token there are 10 possible tokens. Out of then token the has been selected. You can rerun the code and verify weather your predicted token is out of these 10 tokens or not.\n\n\n\nIntroducing randomness alone isn’t always sufficient for generating desired text. When the goal is to extract and present information based on a specific document, prioritizing tokens with the highest probability is often preferred for accuracy and coherence. However, when crafting creative content like blog posts, encouraging the model to explore more diverse and unexpected word choices can lead to richer and more engaging outputs.\nThis balance between predictability and creativity can be effectively controlled using a parameter called temperature. The temperature value adjusts the probability distribution of the predicted tokens. A low temperature makes the distribution sharper, increasing the likelihood of selecting high-probability tokens and thus resulting in more focused and deterministic output. Conversely, a high temperature flattens the probability distribution, giving lower-probability tokens a greater chance of being selected, thereby injecting more randomness and creativity into the generated text.\nLet’s define a custom softmax function that incorporates this temperature parameter:\nimport torch.nn.functional as F\n\n# Defining updated softmax for PyTorch tensors\ndef softmax_tensor(logits: torch.Tensor, temperature: float = 1.0) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the softmax function to a PyTorch tensor along the last dimension,\n    optionally with a temperature scaling factor.\n\n    Args:\n        logits: The input PyTorch tensor of logits.\n        temperature: A scaling factor for the logits (default: 1.0).\n\n    Returns:\n        A PyTorch tensor of the same shape as the input, with probabilities\n        along the last dimension.\n    \"\"\"\n    return F.softmax(logits / temperature, dim=-1)\nUpdating Top-p Search function by including temperature parameter.\ndef top_p_search(prompt, max_length=50, p=1, temperature = 1, show_option = False):\n    \"\"\"\n    Predict next word using top p Search with temperature.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        temperature (float): A number of control randomness.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = softmax_tensor(logits= logits, temperature= temperature + 1e-6)  # Added temperature while calculating probability\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) \n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) \n\n# Example usage with very low temperature\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTokens generated with very low temperature of the model:\n--------------------\nFinal generated Text is:\ni love the idea of a \"super-hero\" who\n--------------------\nConsequently, as you can observe, the initial few tokens closely mirror the results obtained with our earlier deterministic approach. Let’s now explore the effect of increasing the temperature.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you no matter what,\" Trump wrote in a January\n--------------------\nUpon increasing the temperature to 1, we observe that the model begins to select more varied and less predictable words. However, it’s important to note that employing excessively high temperature values can lead to generated text that lacks coherence and meaning. A generally recommended and effective range for the temperature parameter is between 0 and 1.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.7)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you so much.\n\nMORGAN:\n--------------------\nText generated with temperature 0.7 feel more natural. (Try playing with different temperature value)\nYou can also see the list of all possible words by changing show_option parameter to True at different temperature value. ( Try playing with that as well, and see if you observe any pattern among the number of words and temperature value.)\nA special thank you to Rohan-Paul-AI for the inspiration behind this post, and thanks also to Koushik Khan for encouraging me to write it!"
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#greedy-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#greedy-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "When we pass text to a language model and ask it to predict the next word, it calculates the probability of each possible token given the input. The most natural and intuitive approach is to choose the token with the highest probability. This method of choosing the next token is called Greedy Search.\nIn probabilistic terms, the t th token, given the tokens from 1 to t-1, will be \nP\\big(W_t|W_{1:t-1}\\big) = \\argmax_{i} \\bigg[P\\big(W_i|W_{1:t-1}\\big)\\bigg]\n Let’s see how to implement this. First install torch, transformers, and hf_xet.\n!pip install torch transformers hf_xet\nNext, let’s import the required libraries, set up our device, and download the model.\nNow let’s define a custom function that predicts the next token using greedy search. In this function, we first predict the logits score for each token, then convert those to probabilities using the softmax function.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps):\n    \"\"\"\n    Predict next word using Greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n\n        # Returning iterations as df\n        return tokenizer.decode(input_ids[0])\n\nLet us pass i love and predict next few word using our custom function and compare the result with transformers library.\n\ninput_text = \"i love\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\ngenerated_text = greedy_search(input_text=input_text,\n                              input_ids=input_ids,\n                              time_steps=20     # Generating Next 20 tokens\n                              )\nprint(generated_text)\n\nThe output of our custom greedy_search() is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nLet’s predicted tokens using transformers library.\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=20)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\n\nOutput of transformer library is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nGenerated words are exactly same hah!!\nSince this is an deterministic approach predicted words are exactly same.\nIf you want to find out what are the other probable tokens and what is there probability you can try this code.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps, choices_per_step):\n    \"\"\"\n    Predict next word using greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n        choices_per_step (int): Number of choice at each step.\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n\n            # Top few highest tokens\n            for choics_idx in range(choices_per_step):\n                token_index_sorted = sorted_index_of_next_probability[choics_idx]\n                token_prob = next_token_probability[token_index_sorted].cpu().numpy()\n                token_choice = f\"{tokenizer.decode(token_index_sorted)} ({token_prob*100:.2f}%)\"\n                iteration[f'Choice {choics_idx+1}'] = token_choice\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n            iterations.append(iteration)\n\n        # Returning iterations as df\n        return pd.DataFrame(iterations)"
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#beam-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#beam-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "One of the main disadvantage of Greedy Search is, it fails to find out high probable words hidden behind low probability word. To fix this, there’s a smarter method called Beam Search. Beam Search doesn’t just pick one word at a time—it keeps track of a few possible paths and explores them to find the best combination of words.\nLet’s see how it works with an easy example:\n\n\n\nfig-1: Beam Search Example\n\n\nIn the preceding example, we’ve considered the initial tokens i and love The model predicts five potential next tokens, with the exhibiting the highest probability. Consequently, a Greedy search strategy would select the as the subsequent token, followed by the token with the highest probability given the, which in this case is idea.\nBut in Beam search for 2 beams we will be calculating conditional probability of next 2 tokens. The conditional probabilities are:\n\nP('the','way'|'i','love') = 0.25 \\times 0.03 = 0.0075\n \nP('the','idea'|'i','love')=0.25\\times 0.45 = 0.1125\n \nP('the','look'|'i','love')=0.25\\times0.25=0.0625\n ...\n\n\nP('it','.'|'i','love')=0.23\\times0.6=0.138\n\n\n\nP('it','mom'|'i','love')=0.23\\times0.4=0.092\n We can observe, the conditional probability of the sequence it and . is the highest among the considered pairs. Therefore, in Beam Search with a beam width of 2, it will be selected as the first subsequent token, rather than the, despite the having the highest individual probability in the previous step.\nLet’s try to write a function for beam search\n\n# Beam search function\ndef beam_search(input_text, max_length=50, num_beams=5):\n    \"\"\"\n    Predict next word using Beam Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        num_beams (int): Number of Beams to consider.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    # Initialize beams: Each beam starts with the same initial input\n    beams = [(input_ids, 0)]  # Tuple of (tokens, score)\n    \n    for _ in range(max_length):\n        new_beams = []\n        \n        for tokens, score in beams:\n            # Get model predictions\n            with torch.no_grad():\n                outputs = model(tokens)\n            \n            # Extract logits for the last token and apply softmax\n            logits = outputs.logits[:, -1, :]\n            probs = torch.softmax(logits, dim=-1)\n            \n            # Get top beam candidates\n            top_probs, top_indices = probs.topk(num_beams)\n            \n            # Create new beams\n            for i in range(num_beams):\n                new_token = top_indices[:, i].unsqueeze(-1)\n                new_score = score + torch.log(top_probs[:, i])  # Update score\n                \n                new_beam = (torch.cat([tokens, new_token], dim=-1), new_score)\n                new_beams.append(new_beam)\n        \n        # Sort beams by score and keep the best ones\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:num_beams]\n    \n    # Select the best final beam\n    best_tokens = beams[0][0]\n    return tokenizer.decode(best_tokens[0], skip_special_tokens=True)\n\nLet’s Try with the same input i love and then try to generate next 2 tokens using beam_search() function:\n# Example usage\ninput_text = \"i love\"\ngenerated_text = beam_search(input_text, max_length=10, num_beams=2)  # num_beams = 1 is same as greedy search\nprint(generated_text)\ni love it. I love it. I love it.\nOne thing to note, Beam Search with one beam is same as greedy search.(Verify!)\nNow Let’s us compare the result form our function and output from transformers library.\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=10, num_beams = 3)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\ni love it. I love it. I love it.\nInterestingly, the outputs in both scenarios end up being identical. However, a noticeable issue is the repetitiveness of the generated text. This tendency towards repetition is a common drawback of deterministic decoding strategies. To mitigate this, we can introduce an element of randomness into the token selection process."
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-k-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-k-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "To introduce randomness into the token selection process, one effective method is to randomly choose a token from the top k most probable predictions. This technique is known as Top-k Sampling or Top-k Search. The process involves the following steps:\n\nPredict Probabilities: First, the model predicts the probability distribution over all possible tokens.\nSelect Top-k: We then identify and select the k tokens with the highest probabilities.\nNormalize Probabilities: The probabilities of these k selected tokens are re-normalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_k_search(input_text, max_length=50, k=5, show_option= False):\n        \"\"\"\n    Predict next word using top k Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        k (int): Number of high probability token to consider.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      top_probs, top_indices = probs.topk(k)\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=1, k=5, show_option = True)  # num_beams = 1 is same as gready search\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTop 5 most probable tokens are:\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (22.54%)\n it (22.47%)\n you (21.72%)\n to (19.86%)\n this (13.4%)\nThe next selected token is the. At this step, the five most probable predicted tokens are the, it, you, to, and this. From these five, one token will be randomly selected based on their respective probabilities.\nThe subsequent words were generated using the transformers library.\n# set seed to reproduce results. Feel free to change the seed though to get different results\nfrom transformers import set_seed\n\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# set top_k to 50\nsample_output = model.generate(\n    **input_tokens,\n    max_new_tokens=4,\n    do_sample=True,\n    top_k=5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))\ni love the \nIn this instance, the predicted token is also the, although this is purely coincidental. Rerunning the code might yield a different token, but it’s important to note that the subsequent selected token will invariably be one of the five most probable tokens from the preceding output (as you can verify!).\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=10, k=5)\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love the way he's doing this, but I'm\n--------------------\nIf we predict next 10 tokens we can see there is not any repetition to tokens like previous case."
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-p-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-p-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "Another effective method involves considering a dynamic set of the most probable tokens whose cumulative probability exceeds a predefined threshold, p. This technique is known as Top-p Sampling or Nucleus Sampling.\nThe process unfolds as follows:\n\nPredict Probabilities: Initially, the model predicts the probability distribution over all possible tokens.\nIdentify the Nucleus: We then identify the smallest set of most probable tokens such that the sum of their probabilities is greater than or equal to the probability threshold p.\nNormalize Probabilities: The probabilities of the tokens within this selected set (the “nucleus”) are renormalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_p_search(input_text, max_length=50, p=1, show_option= False):\n        \"\"\"\n    Predict next word using top p Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=1, p=0.5, show_option= True)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (17.06%)\n it (17.01%)\n you (16.44%)\n to (15.03%)\n this (10.14%)\n that (7.91%)\n my (5.68%)\n her (3.64%)\n him (3.56%)\n them (3.53%)\n--------------------\nFinal generated Text is:\ni love the\n--------------------\nwith p=0.5 when we predict next token there are 10 possible tokens. Out of then token the has been selected. You can rerun the code and verify weather your predicted token is out of these 10 tokens or not."
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#temperature",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#temperature",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "Introducing randomness alone isn’t always sufficient for generating desired text. When the goal is to extract and present information based on a specific document, prioritizing tokens with the highest probability is often preferred for accuracy and coherence. However, when crafting creative content like blog posts, encouraging the model to explore more diverse and unexpected word choices can lead to richer and more engaging outputs.\nThis balance between predictability and creativity can be effectively controlled using a parameter called temperature. The temperature value adjusts the probability distribution of the predicted tokens. A low temperature makes the distribution sharper, increasing the likelihood of selecting high-probability tokens and thus resulting in more focused and deterministic output. Conversely, a high temperature flattens the probability distribution, giving lower-probability tokens a greater chance of being selected, thereby injecting more randomness and creativity into the generated text.\nLet’s define a custom softmax function that incorporates this temperature parameter:\nimport torch.nn.functional as F\n\n# Defining updated softmax for PyTorch tensors\ndef softmax_tensor(logits: torch.Tensor, temperature: float = 1.0) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the softmax function to a PyTorch tensor along the last dimension,\n    optionally with a temperature scaling factor.\n\n    Args:\n        logits: The input PyTorch tensor of logits.\n        temperature: A scaling factor for the logits (default: 1.0).\n\n    Returns:\n        A PyTorch tensor of the same shape as the input, with probabilities\n        along the last dimension.\n    \"\"\"\n    return F.softmax(logits / temperature, dim=-1)\nUpdating Top-p Search function by including temperature parameter.\ndef top_p_search(prompt, max_length=50, p=1, temperature = 1, show_option = False):\n    \"\"\"\n    Predict next word using top p Search with temperature.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        temperature (float): A number of control randomness.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = softmax_tensor(logits= logits, temperature= temperature + 1e-6)  # Added temperature while calculating probability\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) \n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) \n\n# Example usage with very low temperature\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTokens generated with very low temperature of the model:\n--------------------\nFinal generated Text is:\ni love the idea of a \"super-hero\" who\n--------------------\nConsequently, as you can observe, the initial few tokens closely mirror the results obtained with our earlier deterministic approach. Let’s now explore the effect of increasing the temperature.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you no matter what,\" Trump wrote in a January\n--------------------\nUpon increasing the temperature to 1, we observe that the model begins to select more varied and less predictable words. However, it’s important to note that employing excessively high temperature values can lead to generated text that lacks coherence and meaning. A generally recommended and effective range for the temperature parameter is between 0 and 1.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.7)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you so much.\n\nMORGAN:\n--------------------\nText generated with temperature 0.7 feel more natural. (Try playing with different temperature value)\nYou can also see the list of all possible words by changing show_option parameter to True at different temperature value. ( Try playing with that as well, and see if you observe any pattern among the number of words and temperature value.)\nA special thank you to Rohan-Paul-AI for the inspiration behind this post, and thanks also to Koushik Khan for encouraging me to write it!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sushobhon Karmakar",
    "section": "",
    "text": "Hello! I’m Sushobhon Karmakar, a passionate data scientist with over four years of experience, currently working as Senior Data Scientist at Tiger Analytics.\nHi there! Thanks for visiting my portfolio. I’m a data scientist with over four years of experience, currently serving as a Senior Data Scientist at Tiger Analytics in Bangalore, India.\nI’m thrilled to have you here! This space is where I share my journey, insights, and projects as I explore the fascinating world of data. Whether it’s uncovering patterns or solving complex problems, I’m driven by a love for turning data into meaningful stories.\nThanks for stopping by! I hope you enjoy discovering more about my work.\n\nI’d love to hear from you! Feel free to reach out to me via email at sushobhonkarmakar@gmail.com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "M. Sc. in Statistics (2020), Indian Institute of Technology (IITK), UP, India.\nB. Sc. in Statistics (2017), Visva-Bharati, WB, India."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "M. Sc. in Statistics (2020), Indian Institute of Technology (IITK), UP, India.\nB. Sc. in Statistics (2017), Visva-Bharati, WB, India."
  },
  {
    "objectID": "about.html#career-history",
    "href": "about.html#career-history",
    "title": "About Me",
    "section": " Career History",
    "text": "Career History\n\nSenior Data Scientist at Tiger Analytics (Jan’25 - Present).\nData Scientist at Tiger Analytics (Jun’23 - Dec’24).\nSenior Analyst at Tiger Analytics (Jun’21 - May’23).\nAnalyst at Tiger Analytics (Jan’21 - May’21)"
  },
  {
    "objectID": "about.html#current-responsibilities",
    "href": "about.html#current-responsibilities",
    "title": "About Me",
    "section": " Current Responsibilities",
    "text": "Current Responsibilities\n\nProject Management: Led a cross-functional initiative to enhance business performance visibility. Collaborated closely with stakeholders and ground users, partnering with the Tableau team to develop an intuitive dashboard for actionable segment performance analysis.\nSupporting: Provided hands-on guidance to a Data Scientists and a Scenior Analyst in model development and client engagement, consistently driving the timely delivery of high-quality solutions.\nMentoring: Guided a data scientist’s development, ensuring a balanced approach to learning and work while cultivating the advanced skills crucial for senior data scientist advancement.\nInovation and RnD: Continuously research and integrate cutting-edge data science advancements to enhance existing models. I develop innovative, data-driven solutions and streamline implementation for the Model Refreshing Team, ensuring clients receive timely, optimized outcomes."
  },
  {
    "objectID": "about.html#things-i-love-to-do",
    "href": "about.html#things-i-love-to-do",
    "title": "About Me",
    "section": " Things I Love To Do",
    "text": "Things I Love To Do\n\nReading and listening to Bengali literatures.\nExploring new Technologies.\nCooking Indian Recipies.\nPlaying Football and Badminton."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the list of my personal and academic projects:\n\nWater Quality Prediction: Predicting wather water is portable or not based on pH, Hardness, Solides, Sulfate etc. Used KNN and Random Forest to classify. Github\nPotato Disease Classification: Identifying Potato Plant Disease based on images of potato leaves. Created a front end using HTML and FastApi. Github\n2048: Replicated well-known 2048 game using python.Github\nBreast Cancer Risk Prediction: Predicting Risk of having Breast Cancer based on Age, BMI, Glucose, Insulin etc. Used Logistic Regression to Predict the Risk. Drive"
  },
  {
    "objectID": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html",
    "href": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html",
    "title": "Linear Regression Representated in Neural Net",
    "section": "",
    "text": "Linear regression is one of the most basic and widely used techniques in statistics and machine learning. It helps us understand the relationship between input variables (like temperature, hours studied, etc.) and an output variable (like ice cream sales or exam scores). But did you know that this simple model can also be represented using a neural network?\nAt first glance, neural networks might seem complex—with layers, activations, and weights—but at their core, they’re built on simple mathematical operations. In fact, if we strip away the complexity and keep just the basics, a neural network can behave exactly like a linear regression model.\nIn this blog, we’ll explore how linear regression (both single and multiple input versions) can be expressed as a very simple neural network. This not only helps us understand neural networks better, but also shows how classical models and modern deep learning are more connected than we often realize."
  },
  {
    "objectID": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html#introduction",
    "href": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html#introduction",
    "title": "Linear Regression Representated in Neural Net",
    "section": "",
    "text": "Linear regression is one of the most basic and widely used techniques in statistics and machine learning. It helps us understand the relationship between input variables (like temperature, hours studied, etc.) and an output variable (like ice cream sales or exam scores). But did you know that this simple model can also be represented using a neural network?\nAt first glance, neural networks might seem complex—with layers, activations, and weights—but at their core, they’re built on simple mathematical operations. In fact, if we strip away the complexity and keep just the basics, a neural network can behave exactly like a linear regression model.\nIn this blog, we’ll explore how linear regression (both single and multiple input versions) can be expressed as a very simple neural network. This not only helps us understand neural networks better, but also shows how classical models and modern deep learning are more connected than we often realize."
  },
  {
    "objectID": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html#a-basic-linear-regression-representated-as-neural-network",
    "href": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html#a-basic-linear-regression-representated-as-neural-network",
    "title": "Linear Regression Representated in Neural Net",
    "section": "A Basic Linear Regression Representated as Neural Network",
    "text": "A Basic Linear Regression Representated as Neural Network\nTake a basic Linear Regression Model with p Independent variable and one Dependent Variable. We can represent it as \ny_i = b_0 + b_1 \\times x_{1,i} + b_2 \\times x_{2,i} + ... + b_p \\times x_{p,i} + \\epsilon_i\n\nfor i = 1, 2, ... , n\nIn matrix notation we can write \ny = X^T b + \\epsilon\n\nwhere, dimention of y is 1 \\times n, dimention of b is 1 \\times p and dimention of X is n \\times p\nNow, Let us consider a simple Neural Network with p input and one hidden layer with one node. Like below image\n\n\n\nfig-1: Simple Neural Network with One Node\n\n\nIf we consider weights are b_1, b_2, ... ,b_p, Bias of the Node is b_0 and the activation function for this Node as\n\nf(a_1, a_2, \\dots, a_p) = \\sum_{i=1}^p a_i\n\nThen the output of the neural network will be\n\ny = b_0 + b_1x_1 + b_2x_2 + ... + b_px_p\n\nWhich is same as Multiple Linear Regression."
  },
  {
    "objectID": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html#neural-network-with-multiple-node-and-one-layer",
    "href": "posts/2025-07-25-Neural-Net-Representation-of-Linear-Regression/index.html#neural-network-with-multiple-node-and-one-layer",
    "title": "Linear Regression Representated in Neural Net",
    "section": "Neural Network with Multiple Node and One Layer",
    "text": "Neural Network with Multiple Node and One Layer\nLet’s consider a Neural Network with 3 Nodes and One Layer (Since this is Regression we need a single output Node to combine all the outputs from the first Layer).\n\n\n\nfig-1: Simple Neural Network with Multiple Node\n\n\nWe have p Independent variables and one Dependent variable (Same as above). We have an Activation Function for all Nodes the same as before. Weight for the first Node is defined as b_{1,i} for i=1,2,\\dots,p. Similarly for second Node b_{2,i}.\nWe have already seen output of first Node y_1 can be expressed as a Linear Regression y_1=x^T.b_1. Similarly y_2=x^T.b_2 and y_3=x^T.b_3. Which are 3 different Linear Regressions.\nIn Matrix form we can write: \n\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} = \\begin{bmatrix} b_{1,0} & b_{1,1} & b_{1,2} & \\dots & b_{1,p}\\\\\n                                                                b_{2,0} & b_{2,1} & b_{2,2} & \\dots & b_{2,p}\\\\\n                                                                b_{3,0} & b_{3,1} & b_{3,2} & \\dots & b_{3,p}\\\\\n                                                                \\vdots & \\vdots & \\vdots & & \\vdots\\\\\n                                                                b_{p,0} & b_{p,1} & b_{p,2} & \\dots & b_{p,p}\n                                                \\end{bmatrix}\n                                                \\begin{bmatrix} x_{1}\\\\ x_{2} \\\\ \\vdots \\\\ x_{p} \\end{bmatrix}\n\\\\\n                                                y^* = Wx^*\n\nWhere, W is the weight Matrix of the first Layer.\nIn the Output Node we will combine all the output of the First Layer. If the weights of Output Node is c_1,c_2 and c_3, then by the same Matrix Notation we can write Final Output\n\n\\hat{y} = c_0 + c_1y_1 + c_2y_2 + c_3y_3 = \\begin{bmatrix} c_0 & c_1 & c_2 & c_3 \\end{bmatrix}\n                                            \\begin{bmatrix} 1 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix}\\\\\n                                            \\hat{y} = c^Ty^{**}\\\\\n                                            \\hat{y} = c^TWx\\\\\n                                            \\hat{y} = W^*x\\\\\n Pushing the similar idea we can represent any Neural Network with Linear Activation Function as combination of Multiple Linear Regressions."
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "Revenue Optimizer\n\n Methods\n\nOptimized revenue generation by applying the Method of Moving Asymptotes (MMA) to operate within defined constraints.\n\n Tools\n\nR: Utilized the nloptr package, a powerful tool for nonlinear optimization.\n\n\nForecasting Time Series variable\n\n Methods\n\nAutoRegressive (AR) and Vector AutoRegressive (VAR) Models.\n\n Tools\n\nPython (pandas, numpy, statsmodels)"
  },
  {
    "objectID": "skills.html#senior-data-scientist-tiger-analytics-jan25---present",
    "href": "skills.html#senior-data-scientist-tiger-analytics-jan25---present",
    "title": "Skills",
    "section": "",
    "text": "Revenue Optimizer\n\n Methods\n\nOptimized revenue generation by applying the Method of Moving Asymptotes (MMA) to operate within defined constraints.\n\n Tools\n\nR: Utilized the nloptr package, a powerful tool for nonlinear optimization.\n\n\nForecasting Time Series variable\n\n Methods\n\nAutoRegressive (AR) and Vector AutoRegressive (VAR) Models.\n\n Tools\n\nPython (pandas, numpy, statsmodels)"
  },
  {
    "objectID": "skills.html#data-scientist-tiger-analytics-aug23---dec24",
    "href": "skills.html#data-scientist-tiger-analytics-aug23---dec24",
    "title": "Skills",
    "section": " Data Scientist, Tiger Analytics (Aug’23 - Dec’24)",
    "text": "Data Scientist, Tiger Analytics (Aug’23 - Dec’24)\n\nPrice Optimization through Market Analysis\n\n Methods\n\nAnalyzed market performance using historical Key Performance Indicators (KPIs) to generate a composite score.\nForecasted future KPIs using Time Series Models.\n\n Tools\n\nPython (pandas, numpy) for comprehensive data analysis and manipulation.\nExcel Solver for optimization of weighting factors.\n\n\nRepresentative Property Identification for Market\n\n Methods\n\nPerformed K-Means clustering analysis on property and location attributes to group similar properties.\nApplied Local Outlier Factor (LOF) technique to identify and remove outliers within each cluster.\nCalculated cluster centroids and used cosine similarity to identify representative properties for each market.\n\n Tools\n\nR (data.table, Rlof) for data analysis.\nR (leaflet, mapview) for interactive geospatial visualization and map generation.\n\n\nFinding Optimal Leases for Discussion\n\n Methods\n\nEmployed Fedorev’s Exchange Algorithm to generate optimal hypothetical lease scenarios.\nUtilized cosine similarity to select the most representative and relevant optimal leases.\n\n Tools\n\nR: AlgDesign package for implementing the Federov’s Exchange Algorithm.\nExcel: Developed a macro to automate the generation of an Excel dashboard from tabular data, facilitating clear and concise presentation of results."
  },
  {
    "objectID": "skills.html#senior-analyst-tiger-analytics-jul21---jul23",
    "href": "skills.html#senior-analyst-tiger-analytics-jul21---jul23",
    "title": "Skills",
    "section": " Senior Analyst, Tiger Analytics (Jul’21 - Jul’23)",
    "text": "Senior Analyst, Tiger Analytics (Jul’21 - Jul’23)\n\nClustering of Patients\n\n Methods\n\nPolytomous Variable Latent Class Analysis (poLCA) for patient group identification.\nDecision Tree Classification (rpart) for new patient classification.\n\n Tools\n\nR (poLCA, rpart, openxlsx)\nExcel Dashboard for user-friendly results visualization.\n\n\nWarehouse Rent Prediction for different Win Probability\n\n Methods\n\nLinear Regression, Gradient Boosting Machines (GBM), XGBoost for predicting Rent.\nLogistic Regression (for win probability)\n\n Tools\n\nR (dplyr, gbm, xgboost, ggplot2)\nDataiku"
  },
  {
    "objectID": "skills.html#analyst-tiger-analytics-jan21---jun21",
    "href": "skills.html#analyst-tiger-analytics-jan21---jun21",
    "title": "Skills",
    "section": " Analyst, Tiger Analytics (Jan’21 - Jun’21)",
    "text": "Analyst, Tiger Analytics (Jan’21 - Jun’21)\n\nStatistical Test of Significance\n\n Methods\n\nStatistical t-Test, z-Test, and proportion Test.\n\n Tools\n\ndplyr, tidyverse, spss packages in R."
  },
  {
    "objectID": "skills.html#academics",
    "href": "skills.html#academics",
    "title": "Skills",
    "section": " Academics",
    "text": "Academics\n\nStatistical Simulation (Bayesian Estimation, MCMC)\n\n Tools\n\nR Programming\n\\LaTeX for report generation\n\n\nComputation of Descriptive Statistics for Practical\n\n Tools\n\nC Programming, Excel\nR Markdown for Report Generation."
  },
  {
    "objectID": "index.html#welcome-to-my-portfolio",
    "href": "index.html#welcome-to-my-portfolio",
    "title": "Sushobhon Karmakar",
    "section": "",
    "text": "Hello! I’m Sushobhon Karmakar, a passionate data scientist with over four years of experience, currently working as Senior Data Scientist at Tiger Analytics.\nHi there! Thanks for visiting my portfolio. I’m a data scientist with over four years of experience, currently serving as a Senior Data Scientist at Tiger Analytics in Bangalore, India.\nI’m thrilled to have you here! This space is where I share my journey, insights, and projects as I explore the fascinating world of data. Whether it’s uncovering patterns or solving complex problems, I’m driven by a love for turning data into meaningful stories.\nThanks for stopping by! I hope you enjoy discovering more about my work.\n\nI’d love to hear from you! Feel free to reach out to me via email at sushobhonkarmakar@gmail.com"
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Sushobhon Karmakar",
    "section": "Get in Touch",
    "text": "Get in Touch\nI’d love to hear from you! Feel free to reach out to me via email at sushobhonkarmakar@gmail.com"
  }
]