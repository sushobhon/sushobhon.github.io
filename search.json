[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling\n\n\n\nGen AI\n\nLLM\n\n\n\nMethods LLM models used to select next token based on some given information\n\n\n\n\n\nJul 1, 2025\n\n\nSushobhon Karmakar\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the list of my personal and academic projects:\n\nWater Quality Prediction: Predicting wather water is portable or not based on pH, Hardness, Solides, Sulfate etc. Used KNN and Random Forest to classify. Github\nPotato Disease Classification: Identifying Potato Plant Disease based on images of potato leaves. Created a front end using HTML and FastApi. Github\n2048: Replicated well-known 2048 game using python.Github\nBreast Cancer Risk Prediction: Predicting Risk of having Breast Cancer based on Age, BMI, Glucose, Insulin etc. Used Logistic Regression to Predict the Risk. Drive"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "Revenue Optimizer\n\n Methods\n\nApplied Method of Moving Asymptotes (MMA) to maximize revenue subjected to some given constrain\n\n Tools\n\nR: Utilized the nloptr package, a powerful tool for nonlinear optimization\n\n\nForecasting Time Series variable\n\n Methods\n\nAuto Regressive (AR) and Vector Auto Regressive (VAR) Model\n\n Tools\n\nPython (pandas, numpy, statsmpdels)"
  },
  {
    "objectID": "skills.html#senior-data-scientist-tiger-analytics-jan25---present",
    "href": "skills.html#senior-data-scientist-tiger-analytics-jan25---present",
    "title": "Skills",
    "section": "",
    "text": "Revenue Optimizer\n\n Methods\n\nApplied Method of Moving Asymptotes (MMA) to maximize revenue subjected to some given constrain\n\n Tools\n\nR: Utilized the nloptr package, a powerful tool for nonlinear optimization\n\n\nForecasting Time Series variable\n\n Methods\n\nAuto Regressive (AR) and Vector Auto Regressive (VAR) Model\n\n Tools\n\nPython (pandas, numpy, statsmpdels)"
  },
  {
    "objectID": "skills.html#data-scientist-tiger-analytics-aug23---dec24",
    "href": "skills.html#data-scientist-tiger-analytics-aug23---dec24",
    "title": "Skills",
    "section": " Data Scientist, Tiger Analytics (Aug’23 - Dec’24)",
    "text": "Data Scientist, Tiger Analytics (Aug’23 - Dec’24)\n\nPrice Optimization through Market Analysis\n\n Methods\n\nAnalyzed market performance using historical Key Performance Indicators (KPIs) to generate a composite score\nForecasted Future KPI’s using Time Series Models\n\n Tools\n\nPython (pandas, numpy) for comprehensive data analysis and manipulation\nExcel Solver for optimization of weighting factors\n\n\nRepresentative Property Identification for Market\n\n Methods\n\nPerformed KMeans clustering analysis on property and location attributes to group similar properties\nApplied Local Outlier Factor (LOF) technique to identify and remove outliers within each cluster\nCalculated cluster centroids and used cosine similarity to identify representative properties for each market\n\n Tools\n\nR (data.table, Rlof) for data analysis\nR (leaflet, mapview) for interactive geospatial visualization and map generation\n\n\nFinding Optimal Leases for Discussion\n\n Methods\n\nEmployed Federov’s Exchange Algorithm to generate optimal hypothetical lease scenarios\nUtilized cosine similarity to select the most representative and relevant optimal leases\n\n Tools\n\nR: AlgDesign package for implementing the Federov’s Exchange Algorithm\nExcel: Developed a macro to automate the generation of an Excel dashboard from tabular data, facilitating clear and concise presentation of results"
  },
  {
    "objectID": "skills.html#senior-analyst-tiger-analytics-jul21---jul23",
    "href": "skills.html#senior-analyst-tiger-analytics-jul21---jul23",
    "title": "Skills",
    "section": " Senior Analyst, Tiger Analytics (Jul’21 - Jul’23)",
    "text": "Senior Analyst, Tiger Analytics (Jul’21 - Jul’23)\n\nClustering of Patients\n\n Methods\n\nPolytomous Variable Latent Class Analysis (poLCA) for patient group identification.\nDecision Tree Classification (rpart) for new patient classification.\n\n Tools\n\nR (poLCA, rpart, openxlsx)\nExcel Dashboard for user-friendly results visualization.\n\n\nWarehouse Rent Prediction for different Win Probability\n\n Methods\n\nLinear Regression, Gradient Boosting Machines (GBM), XGBoost for predicting Rent.\nLogistic Regression (for win probability)\n\n Tools\n\nR (dplyr, gbm, xgboost, ggplot2)\nDataiku"
  },
  {
    "objectID": "skills.html#analyst-tiger-analytics-jan21---jun21",
    "href": "skills.html#analyst-tiger-analytics-jan21---jun21",
    "title": "Skills",
    "section": " Analyst, Tiger Analytics (Jan’21 - Jun’21)",
    "text": "Analyst, Tiger Analytics (Jan’21 - Jun’21)\n\nStatistical Test of Significance\n\n Methods\n\nStatistical t-Test, z-Test and proportion Test\n\n Tools\n\ndplyr, tidyverse, spss packages in R Programming"
  },
  {
    "objectID": "skills.html#academics",
    "href": "skills.html#academics",
    "title": "Skills",
    "section": " Academics",
    "text": "Academics\n\nStatistical Simulation (Basyesian Estimation, MCMC)\n\n Tools\n\nR Programming\n\\LaTeX for report generation\n\n\nComputation of Descriptive Statistics for Practical\n\n Tools\n\nC Programming, Excel\nR Markdown for Report Generation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "M. Sc. in Statistics (2020), Indian Institude of Statistics (IITK), UP, India.\nB. Sc. in Statistics (2017), Visva-Bharati, WB, India."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "M. Sc. in Statistics (2020), Indian Institude of Statistics (IITK), UP, India.\nB. Sc. in Statistics (2017), Visva-Bharati, WB, India."
  },
  {
    "objectID": "about.html#career-history",
    "href": "about.html#career-history",
    "title": "About Me",
    "section": " Career History",
    "text": "Career History\n\nSenior Data Scientist at Tiger Analytics (Jan’25 - Present).\nData Scientist at Tiger Analytics (Jun’23 - Dec’24).\nSenior Analyst at Tiger Analytics (Jun’21 - May’23).\nAnalyst at Tiger Analytics (Jan’21 - May’21)"
  },
  {
    "objectID": "about.html#current-responsibilities",
    "href": "about.html#current-responsibilities",
    "title": "About Me",
    "section": " Current Responsibilities",
    "text": "Current Responsibilities\n\nProject Management: Leading a Project to Monitor Business Performance. Identify which segment is Under-Performing / Over-Performing. Collaborated with Stakeholders, Ground Users and Tableau Team to Build a Dashboard for easy access.\nSupporting: Guiding a Data Scientist on Model Building, helping with next stapes, to interact with Client, and ensuring quality deliverables on time.\nMentoring: Mentoring a Data Scientist, ensuring his learning, work-life balance. Guiding him to aquire new skils required to become Senior Data Scientist.\nInovation and RnD: Stay up-to-date with the latest advancements in Data Science. Implementing new technologies and menthods to imporve the existing Models and Provinding easy to implement solutions to Model Refreshing Team. Cumming up Data Driven Solutions time to time for Client."
  },
  {
    "objectID": "about.html#things-i-love-to-do",
    "href": "about.html#things-i-love-to-do",
    "title": "About Me",
    "section": " Things I Love To Do:",
    "text": "Things I Love To Do:\n\nReading and Listening Bengali Books.\nCooking Indian Recipies.\nPlaying Football and Badminton."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sushobhon Karmakar",
    "section": "",
    "text": "Discover My Work\nHello! I’m Sushobhon Karmakar, a passionate data scientist with over four years of experience, currently working as Senior Data Scientist at Tiger Analytics.\n\n\nIntroduction\nHi there! Thanks for visiting my portfolio. I’m a data scientist with over four years of experience, currently serving as a Senior Data Scientist at Tiger Analytics in Bangalore, India.\nI’m thrilled to have you here! This space is where I share my journey, insights, and projects as I explore the fascinating world of data. Whether it’s uncovering patterns or solving complex problems, I’m driven by a love for turning data into meaningful stories.\nThanks for stopping by! I hope you enjoy discovering more about my work.\n\n\nGet in Touch\nI’d love to hear from you! Feel free to reach out to me via email at sushobhonkarmakar@gmail.com"
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "Have you ever wondered how your phone magically suggests the next word as you type? Or how chatbots string together coherent sentences? The secret lies in something called next token generation.\nThink of it this way: imagine telling a story one word at a time. After saying “The big,” what might come next? Perhaps “dog,” “house,” or “tree.” Next token generation teaches computers to do exactly this — predict the most likely word (or sometimes a piece of a word, called a “token”) that should follow the current sequence.\nFor instance, if you type “Thank you for your,” a next token generation model might suggest “help” as the most likely next word. It’s like having a super-smart autocomplete feature!\nIn this blog, we’ll pull back the curtain to explore the fascinating techniques behind this technology. We’ll dive into some code to see how it works and compare our results with the leading transformer libraries.\nGet ready to explore the world of language prediction, where we’ll examine five popular methods that make next token generation possible: Greedy Search, Beam Search, Top-k Sampling, Top-p (Nucleus) Sampling, and Temperature Control. By the end, you’ll understand how machines learn to speak our language!\nIn this blog, we’ll use the gpt2-medium model to predict token probabilities, though you’re welcome to experiment with other models.\n\n\nWhen we pass text to a language model and ask it to predict the next word, it calculates the probability of each possible token given the input. The most natural and intuitive approach is to choose the token with the highest probability. This method of choosing the next token is called Greedy Search.\nIn probabilistic terms, the t th token, given the tokens from 1 to t-1, will be \nP\\big(W_t|W_{1:t-1}\\big) = \\argmax_{i} \\bigg[P\\big(W_i|W_{1:t-1}\\big)\\bigg]\n Let’s see how to implement this. First install torch, transformers, and hf_xet.\n!pip install torch transformers hf_xet\nNext, let’s import the required libraries, set up our device, and download the model.\nNow let’s define a custom function that predicts the next token using greedy search. In this function, we first predict the logits score for each token, then convert those to probabilities using the softmax function.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps):\n    \"\"\"\n    Predict next word using Greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n\n        # Returning iterations as df\n        return tokenizer.decode(input_ids[0])\n\nLet us pass i love and predict next few word using our custom function and compare the result with transformers library.\n\ninput_text = \"i love\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\ngenerated_text = greedy_search(input_text=input_text,\n                              input_ids=input_ids,\n                              time_steps=20     # Generating Next 20 tokens\n                              )\nprint(generated_text)\n\nThe output of our custom greedy_search() is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nLet’s predicted tokens using transformers library.\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=20)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\n\nOutput of transformer library is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nGenerated words are exactly same hah!!\nSince this is an deterministic approach predicted words are exactly same.\nIf you want to find out what are the other probable tokens and what is there probability you can try this code.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps, choices_per_step):\n    \"\"\"\n    Predict next word using greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n        choices_per_step (int): Number of choice at each step.\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n\n            # Top few highest tokens\n            for choics_idx in range(choices_per_step):\n                token_index_sorted = sorted_index_of_next_probability[choics_idx]\n                token_prob = next_token_probability[token_index_sorted].cpu().numpy()\n                token_choice = f\"{tokenizer.decode(token_index_sorted)} ({token_prob*100:.2f}%)\"\n                iteration[f'Choice {choics_idx+1}'] = token_choice\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n            iterations.append(iteration)\n\n        # Returning iterations as df\n        return pd.DataFrame(iterations)\n\n\n\n\nOne of the main disadvantage of Greedy Search is, it fails to find out high probable words hidden behind low probability word. To fix this, there’s a smarter method called Beam Search. Beam Search doesn’t just pick one word at a time—it keeps track of a few possible paths and explores them to find the best combination of words.\nLet’s see how it works with an easy example:\n\n\n\nfig-1: Beam Search Example\n\n\nIn the preceding example, we’ve considered the initial tokens i and love The model predicts five potential next tokens, with the exhibiting the highest probability. Consequently, a Greedy search strategy would select the as the subsequent token, followed by the token with the highest probability given the, which in this case is idea.\nBut in Beam search for 2 beams we will be calculating conditional probability of next 2 tokens. The conditional probabilities are:\n\nP('the','way'|'i','love') = 0.25 \\times 0.03 = 0.0075\n \nP('the','idea'|'i','love')=0.25\\times 0.45 = 0.1125\n \nP('the','look'|'i','love')=0.25\\times0.25=0.0625\n ...\n\n\nP('it','.'|'i','love')=0.23\\times0.6=0.138\n\n\n\nP('it','mom'|'i','love')=0.23\\times0.4=0.092\n We can observe, the conditional probability of the sequence it and . is the highest among the considered pairs. Therefore, in Beam Search with a beam width of 2, it will be selected as the first subsequent token, rather than the, despite the having the highest individual probability in the previous step.\nLet’s try to write a function for beam search\n\n# Beam search function\ndef beam_search(input_text, max_length=50, num_beams=5):\n    \"\"\"\n    Predict next word using Beam Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        num_beams (int): Number of Beams to consider.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    # Initialize beams: Each beam starts with the same initial input\n    beams = [(input_ids, 0)]  # Tuple of (tokens, score)\n    \n    for _ in range(max_length):\n        new_beams = []\n        \n        for tokens, score in beams:\n            # Get model predictions\n            with torch.no_grad():\n                outputs = model(tokens)\n            \n            # Extract logits for the last token and apply softmax\n            logits = outputs.logits[:, -1, :]\n            probs = torch.softmax(logits, dim=-1)\n            \n            # Get top beam candidates\n            top_probs, top_indices = probs.topk(num_beams)\n            \n            # Create new beams\n            for i in range(num_beams):\n                new_token = top_indices[:, i].unsqueeze(-1)\n                new_score = score + torch.log(top_probs[:, i])  # Update score\n                \n                new_beam = (torch.cat([tokens, new_token], dim=-1), new_score)\n                new_beams.append(new_beam)\n        \n        # Sort beams by score and keep the best ones\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:num_beams]\n    \n    # Select the best final beam\n    best_tokens = beams[0][0]\n    return tokenizer.decode(best_tokens[0], skip_special_tokens=True)\n\nLet’s Try with the same input i love and then try to generate next 2 tokens using beam_search() function:\n# Example usage\ninput_text = \"i love\"\ngenerated_text = beam_search(input_text, max_length=10, num_beams=2)  # num_beams = 1 is same as greedy search\nprint(generated_text)\ni love it. I love it. I love it.\nOne thing to note, Beam Search with one beam is same as greedy search.(Verify!)\nNow Let’s us compare the result form our function and output from transformers library.\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=10, num_beams = 3)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\ni love it. I love it. I love it.\nInterestingly, the outputs in both scenarios end up being identical. However, a noticeable issue is the repetitiveness of the generated text. This tendency towards repetition is a common drawback of deterministic decoding strategies. To mitigate this, we can introduce an element of randomness into the token selection process.\n\n\n\nTo introduce randomness into the token selection process, one effective method is to randomly choose a token from the top k most probable predictions. This technique is known as Top-k Sampling or Top-k Search. The process involves the following steps:\n\nPredict Probabilities: First, the model predicts the probability distribution over all possible tokens.\nSelect Top-k: We then identify and select the k tokens with the highest probabilities.\nNormalize Probabilities: The probabilities of these k selected tokens are re-normalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_k_search(input_text, max_length=50, k=5, show_option= False):\n        \"\"\"\n    Predict next word using top k Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        k (int): Number of high probability token to consider.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      top_probs, top_indices = probs.topk(k)\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=1, k=5, show_option = True)  # num_beams = 1 is same as gready search\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTop 5 most probable tokens are:\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (22.54%)\n it (22.47%)\n you (21.72%)\n to (19.86%)\n this (13.4%)\nThe next selected token is the. At this step, the five most probable predicted tokens are the, it, you, to, and this. From these five, one token will be randomly selected based on their respective probabilities.\nThe subsequent words were generated using the transformers library.\n# set seed to reproduce results. Feel free to change the seed though to get different results\nfrom transformers import set_seed\n\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# set top_k to 50\nsample_output = model.generate(\n    **input_tokens,\n    max_new_tokens=4,\n    do_sample=True,\n    top_k=5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))\ni love the \nIn this instance, the predicted token is also the, although this is purely coincidental. Rerunning the code might yield a different token, but it’s important to note that the subsequent selected token will invariably be one of the five most probable tokens from the preceding output (as you can verify!).\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=10, k=5)\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love the way he's doing this, but I'm\n--------------------\nIf we predict next 10 tokens we can see there is not any repetition to tokens like previous case.\n\n\n\nAnother effective method involves considering a dynamic set of the most probable tokens whose cumulative probability exceeds a predefined threshold, p. This technique is known as Top-p Sampling or Nucleus Sampling.\nThe process unfolds as follows:\n\nPredict Probabilities: Initially, the model predicts the probability distribution over all possible tokens.\nIdentify the Nucleus: We then identify the smallest set of most probable tokens such that the sum of their probabilities is greater than or equal to the probability threshold p.\nNormalize Probabilities: The probabilities of the tokens within this selected set (the “nucleus”) are renormalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_p_search(input_text, max_length=50, p=1, show_option= False):\n        \"\"\"\n    Predict next word using top p Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=1, p=0.5, show_option= True)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (17.06%)\n it (17.01%)\n you (16.44%)\n to (15.03%)\n this (10.14%)\n that (7.91%)\n my (5.68%)\n her (3.64%)\n him (3.56%)\n them (3.53%)\n--------------------\nFinal generated Text is:\ni love the\n--------------------\nwith p=0.5 when we predict next token there are 10 possible tokens. Out of then token the has been selected. You can rerun the code and verify weather your predicted token is out of these 10 tokens or not.\n\n\n\nIntroducing randomness alone isn’t always sufficient for generating desired text. When the goal is to extract and present information based on a specific document, prioritizing tokens with the highest probability is often preferred for accuracy and coherence. However, when crafting creative content like blog posts, encouraging the model to explore more diverse and unexpected word choices can lead to richer and more engaging outputs.\nThis balance between predictability and creativity can be effectively controlled using a parameter called temperature. The temperature value adjusts the probability distribution of the predicted tokens. A low temperature makes the distribution sharper, increasing the likelihood of selecting high-probability tokens and thus resulting in more focused and deterministic output. Conversely, a high temperature flattens the probability distribution, giving lower-probability tokens a greater chance of being selected, thereby injecting more randomness and creativity into the generated text.\nLet’s define a custom softmax function that incorporates this temperature parameter:\nimport torch.nn.functional as F\n\n# Defining updated softmax for PyTorch tensors\ndef softmax_tensor(logits: torch.Tensor, temperature: float = 1.0) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the softmax function to a PyTorch tensor along the last dimension,\n    optionally with a temperature scaling factor.\n\n    Args:\n        logits: The input PyTorch tensor of logits.\n        temperature: A scaling factor for the logits (default: 1.0).\n\n    Returns:\n        A PyTorch tensor of the same shape as the input, with probabilities\n        along the last dimension.\n    \"\"\"\n    return F.softmax(logits / temperature, dim=-1)\nUpdating Top-p Search function by including temperature parameter.\ndef top_p_search(prompt, max_length=50, p=1, temperature = 1, show_option = False):\n    \"\"\"\n    Predict next word using top p Search with temperature.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        temperature (float): A number of control randomness.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = softmax_tensor(logits= logits, temperature= temperature + 1e-6)  # Added temperature while calculating probability\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) \n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) \n\n# Example usage with very low temperature\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTokens generated with very low temperature of the model:\n--------------------\nFinal generated Text is:\ni love the idea of a \"super-hero\" who\n--------------------\nConsequently, as you can observe, the initial few tokens closely mirror the results obtained with our earlier deterministic approach. Let’s now explore the effect of increasing the temperature.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you no matter what,\" Trump wrote in a January\n--------------------\nUpon increasing the temperature to 1, we observe that the model begins to select more varied and less predictable words. However, it’s important to note that employing excessively high temperature values can lead to generated text that lacks coherence and meaning. A generally recommended and effective range for the temperature parameter is between 0 and 1.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.7)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you so much.\n\nMORGAN:\n--------------------\nText generated with temperature 0.7 feel more natural. (Try playing with different temperature value)\nYou can also see the list of all possible words by changing show_option parameter to True at different temperature value. ( Try playing with that as well, and see if you observe any pattern among the number of words and temperature value.)\nA special thank you to Rohan-Paul-AI for the inspiration behind this post, and thanks also to Koushik Khan for encouraging me to write it!"
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#greedy-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#greedy-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "When we pass text to a language model and ask it to predict the next word, it calculates the probability of each possible token given the input. The most natural and intuitive approach is to choose the token with the highest probability. This method of choosing the next token is called Greedy Search.\nIn probabilistic terms, the t th token, given the tokens from 1 to t-1, will be \nP\\big(W_t|W_{1:t-1}\\big) = \\argmax_{i} \\bigg[P\\big(W_i|W_{1:t-1}\\big)\\bigg]\n Let’s see how to implement this. First install torch, transformers, and hf_xet.\n!pip install torch transformers hf_xet\nNext, let’s import the required libraries, set up our device, and download the model.\nNow let’s define a custom function that predicts the next token using greedy search. In this function, we first predict the logits score for each token, then convert those to probabilities using the softmax function.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps):\n    \"\"\"\n    Predict next word using Greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n\n        # Returning iterations as df\n        return tokenizer.decode(input_ids[0])\n\nLet us pass i love and predict next few word using our custom function and compare the result with transformers library.\n\ninput_text = \"i love\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\ngenerated_text = greedy_search(input_text=input_text,\n                              input_ids=input_ids,\n                              time_steps=20     # Generating Next 20 tokens\n                              )\nprint(generated_text)\n\nThe output of our custom greedy_search() is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nLet’s predicted tokens using transformers library.\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=20)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\n\nOutput of transformer library is\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\nGenerated words are exactly same hah!!\nSince this is an deterministic approach predicted words are exactly same.\nIf you want to find out what are the other probable tokens and what is there probability you can try this code.\n\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps, choices_per_step):\n    \"\"\"\n    Predict next word using greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n        choices_per_step (int): Number of choice at each step.\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n\n            # Top few highest tokens\n            for choics_idx in range(choices_per_step):\n                token_index_sorted = sorted_index_of_next_probability[choics_idx]\n                token_prob = next_token_probability[token_index_sorted].cpu().numpy()\n                token_choice = f\"{tokenizer.decode(token_index_sorted)} ({token_prob*100:.2f}%)\"\n                iteration[f'Choice {choics_idx+1}'] = token_choice\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n            iterations.append(iteration)\n\n        # Returning iterations as df\n        return pd.DataFrame(iterations)"
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#beam-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#beam-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "One of the main disadvantage of Greedy Search is, it fails to find out high probable words hidden behind low probability word. To fix this, there’s a smarter method called Beam Search. Beam Search doesn’t just pick one word at a time—it keeps track of a few possible paths and explores them to find the best combination of words.\nLet’s see how it works with an easy example:\n\n\n\nfig-1: Beam Search Example\n\n\nIn the preceding example, we’ve considered the initial tokens i and love The model predicts five potential next tokens, with the exhibiting the highest probability. Consequently, a Greedy search strategy would select the as the subsequent token, followed by the token with the highest probability given the, which in this case is idea.\nBut in Beam search for 2 beams we will be calculating conditional probability of next 2 tokens. The conditional probabilities are:\n\nP('the','way'|'i','love') = 0.25 \\times 0.03 = 0.0075\n \nP('the','idea'|'i','love')=0.25\\times 0.45 = 0.1125\n \nP('the','look'|'i','love')=0.25\\times0.25=0.0625\n ...\n\n\nP('it','.'|'i','love')=0.23\\times0.6=0.138\n\n\n\nP('it','mom'|'i','love')=0.23\\times0.4=0.092\n We can observe, the conditional probability of the sequence it and . is the highest among the considered pairs. Therefore, in Beam Search with a beam width of 2, it will be selected as the first subsequent token, rather than the, despite the having the highest individual probability in the previous step.\nLet’s try to write a function for beam search\n\n# Beam search function\ndef beam_search(input_text, max_length=50, num_beams=5):\n    \"\"\"\n    Predict next word using Beam Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        num_beams (int): Number of Beams to consider.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    # Initialize beams: Each beam starts with the same initial input\n    beams = [(input_ids, 0)]  # Tuple of (tokens, score)\n    \n    for _ in range(max_length):\n        new_beams = []\n        \n        for tokens, score in beams:\n            # Get model predictions\n            with torch.no_grad():\n                outputs = model(tokens)\n            \n            # Extract logits for the last token and apply softmax\n            logits = outputs.logits[:, -1, :]\n            probs = torch.softmax(logits, dim=-1)\n            \n            # Get top beam candidates\n            top_probs, top_indices = probs.topk(num_beams)\n            \n            # Create new beams\n            for i in range(num_beams):\n                new_token = top_indices[:, i].unsqueeze(-1)\n                new_score = score + torch.log(top_probs[:, i])  # Update score\n                \n                new_beam = (torch.cat([tokens, new_token], dim=-1), new_score)\n                new_beams.append(new_beam)\n        \n        # Sort beams by score and keep the best ones\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:num_beams]\n    \n    # Select the best final beam\n    best_tokens = beams[0][0]\n    return tokenizer.decode(best_tokens[0], skip_special_tokens=True)\n\nLet’s Try with the same input i love and then try to generate next 2 tokens using beam_search() function:\n# Example usage\ninput_text = \"i love\"\ngenerated_text = beam_search(input_text, max_length=10, num_beams=2)  # num_beams = 1 is same as greedy search\nprint(generated_text)\ni love it. I love it. I love it.\nOne thing to note, Beam Search with one beam is same as greedy search.(Verify!)\nNow Let’s us compare the result form our function and output from transformers library.\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=10, num_beams = 3)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\ni love it. I love it. I love it.\nInterestingly, the outputs in both scenarios end up being identical. However, a noticeable issue is the repetitiveness of the generated text. This tendency towards repetition is a common drawback of deterministic decoding strategies. To mitigate this, we can introduce an element of randomness into the token selection process."
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-k-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-k-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "To introduce randomness into the token selection process, one effective method is to randomly choose a token from the top k most probable predictions. This technique is known as Top-k Sampling or Top-k Search. The process involves the following steps:\n\nPredict Probabilities: First, the model predicts the probability distribution over all possible tokens.\nSelect Top-k: We then identify and select the k tokens with the highest probabilities.\nNormalize Probabilities: The probabilities of these k selected tokens are re-normalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_k_search(input_text, max_length=50, k=5, show_option= False):\n        \"\"\"\n    Predict next word using top k Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        k (int): Number of high probability token to consider.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      top_probs, top_indices = probs.topk(k)\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=1, k=5, show_option = True)  # num_beams = 1 is same as gready search\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTop 5 most probable tokens are:\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (22.54%)\n it (22.47%)\n you (21.72%)\n to (19.86%)\n this (13.4%)\nThe next selected token is the. At this step, the five most probable predicted tokens are the, it, you, to, and this. From these five, one token will be randomly selected based on their respective probabilities.\nThe subsequent words were generated using the transformers library.\n# set seed to reproduce results. Feel free to change the seed though to get different results\nfrom transformers import set_seed\n\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# set top_k to 50\nsample_output = model.generate(\n    **input_tokens,\n    max_new_tokens=4,\n    do_sample=True,\n    top_k=5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))\ni love the \nIn this instance, the predicted token is also the, although this is purely coincidental. Rerunning the code might yield a different token, but it’s important to note that the subsequent selected token will invariably be one of the five most probable tokens from the preceding output (as you can verify!).\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=10, k=5)\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love the way he's doing this, but I'm\n--------------------\nIf we predict next 10 tokens we can see there is not any repetition to tokens like previous case."
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-p-search",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#top-p-search",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "Another effective method involves considering a dynamic set of the most probable tokens whose cumulative probability exceeds a predefined threshold, p. This technique is known as Top-p Sampling or Nucleus Sampling.\nThe process unfolds as follows:\n\nPredict Probabilities: Initially, the model predicts the probability distribution over all possible tokens.\nIdentify the Nucleus: We then identify the smallest set of most probable tokens such that the sum of their probabilities is greater than or equal to the probability threshold p.\nNormalize Probabilities: The probabilities of the tokens within this selected set (the “nucleus”) are renormalized to create a new probability distribution.\nRandom Selection: Finally, a token is randomly chosen from this renormalized distribution.\n\nLet’s illustrate this with a Python function:\ndef top_p_search(input_text, max_length=50, p=1, show_option= False):\n        \"\"\"\n    Predict next word using top p Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=1, p=0.5, show_option= True)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (17.06%)\n it (17.01%)\n you (16.44%)\n to (15.03%)\n this (10.14%)\n that (7.91%)\n my (5.68%)\n her (3.64%)\n him (3.56%)\n them (3.53%)\n--------------------\nFinal generated Text is:\ni love the\n--------------------\nwith p=0.5 when we predict next token there are 10 possible tokens. Out of then token the has been selected. You can rerun the code and verify weather your predicted token is out of these 10 tokens or not."
  },
  {
    "objectID": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#temperature",
    "href": "posts/2025-07-01-Your-Own-Next-Token-Generator/index.html#temperature",
    "title": "The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling",
    "section": "",
    "text": "Introducing randomness alone isn’t always sufficient for generating desired text. When the goal is to extract and present information based on a specific document, prioritizing tokens with the highest probability is often preferred for accuracy and coherence. However, when crafting creative content like blog posts, encouraging the model to explore more diverse and unexpected word choices can lead to richer and more engaging outputs.\nThis balance between predictability and creativity can be effectively controlled using a parameter called temperature. The temperature value adjusts the probability distribution of the predicted tokens. A low temperature makes the distribution sharper, increasing the likelihood of selecting high-probability tokens and thus resulting in more focused and deterministic output. Conversely, a high temperature flattens the probability distribution, giving lower-probability tokens a greater chance of being selected, thereby injecting more randomness and creativity into the generated text.\nLet’s define a custom softmax function that incorporates this temperature parameter:\nimport torch.nn.functional as F\n\n# Defining updated softmax for PyTorch tensors\ndef softmax_tensor(logits: torch.Tensor, temperature: float = 1.0) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the softmax function to a PyTorch tensor along the last dimension,\n    optionally with a temperature scaling factor.\n\n    Args:\n        logits: The input PyTorch tensor of logits.\n        temperature: A scaling factor for the logits (default: 1.0).\n\n    Returns:\n        A PyTorch tensor of the same shape as the input, with probabilities\n        along the last dimension.\n    \"\"\"\n    return F.softmax(logits / temperature, dim=-1)\nUpdating Top-p Search function by including temperature parameter.\ndef top_p_search(prompt, max_length=50, p=1, temperature = 1, show_option = False):\n    \"\"\"\n    Predict next word using top p Search with temperature.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        temperature (float): A number of control randomness.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = softmax_tensor(logits= logits, temperature= temperature + 1e-6)  # Added temperature while calculating probability\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob&lt;=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) \n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) \n\n# Example usage with very low temperature\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\nTokens generated with very low temperature of the model:\n--------------------\nFinal generated Text is:\ni love the idea of a \"super-hero\" who\n--------------------\nConsequently, as you can observe, the initial few tokens closely mirror the results obtained with our earlier deterministic approach. Let’s now explore the effect of increasing the temperature.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you no matter what,\" Trump wrote in a January\n--------------------\nUpon increasing the temperature to 1, we observe that the model begins to select more varied and less predictable words. However, it’s important to note that employing excessively high temperature values can lead to generated text that lacks coherence and meaning. A generally recommended and effective range for the temperature parameter is between 0 and 1.\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.7)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n--------------------\nFinal generated Text is:\ni love you so much.\n\nMORGAN:\n--------------------\nText generated with temperature 0.7 feel more natural. (Try playing with different temperature value)\nYou can also see the list of all possible words by changing show_option parameter to True at different temperature value. ( Try playing with that as well, and see if you observe any pattern among the number of words and temperature value.)\nA special thank you to Rohan-Paul-AI for the inspiration behind this post, and thanks also to Koushik Khan for encouraging me to write it!"
  }
]