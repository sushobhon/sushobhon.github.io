{
  "hash": "0df599f3602f997c26452a07942c8246",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The Art of AI Text: How LLMs Choose Next Word using Greedy, Beam Search, and Sampling\"\nfrom: markdown+emoji\ndescription: \"Methods LLM models used to select next token based on some given information\"\nauthor:\n  - name: Sushobhon Karmakar \ndate: 07-01-2025\ncategories: [Gen AI, LLM]\nimage: Best-Free-AI-Image-Generator-738622791.jpg\ndraft: False\nexecute:\n  eval: false  # This is the key line\n  echo: true   # This ensures the code blocks are still displayed\n---\n\n\n# Intorduction\nHave you ever wondered how your phone magically suggests the next word as you type? Or how chatbots string together coherent sentences? The secret lies in something called next token generation.\n\nThink of it this way: imagine telling a story one word at a time. After saying “The big,” what might come next? Perhaps “dog,” “house,” or “tree.” Next token generation teaches computers to do exactly this — predict the most likely word (or sometimes a piece of a word, called a “token”) that should follow the current sequence.\n\nFor instance, if you type “Thank you for your,” a next token generation model might suggest “help” as the most likely next word. It’s like having a super-smart autocomplete feature!\n\nIn this blog, we’ll pull back the curtain to explore the fascinating techniques behind this technology. We’ll dive into some code to see how it works and compare our results with the leading transformer libraries.\n\nGet ready to explore the world of language prediction, where we’ll examine five popular methods that make next token generation possible: Greedy Search, Beam Search, Top-k Sampling, Top-p (Nucleus) Sampling, and Temperature Control. By the end, you’ll understand how machines learn to speak our language!\n\n*In this blog, we’ll use the `gpt2-medium` model to predict token probabilities, though you’re welcome to experiment with other models.*\n\n## Greedy Search\nWhen we pass text to a language model and ask it to predict the next word, it calculates the probability of each possible token given the input. The most natural and intuitive approach is to choose the token with the highest probability. This method of choosing the next token is called Greedy Search.\n\nIn probabilistic terms, the `t` th token, given the tokens from `1` to `t-1`, will be\n$$\nP\\big(W_t|W_{1:t-1}\\big) = \\argmax_{i} \\bigg[P\\big(W_i|W_{1:t-1}\\big)\\bigg]\n$$\nLet’s see how to implement this. First install `torch`, `transformers`, and `hf_xet`.\n\n```{bash}\n!pip install torch transformers hf_xet\n```\n\nNext, let’s import the required libraries, set up our device, and download the model.\n\n\n\nNow let's define a custom function that predicts the next token using *greedy search*. In this function, we first predict the `logits` score for each token, then convert those to probabilities using the `softmax` function. \n\n::: {#2c63e092 .cell execution_count=2}\n``` {.python .cell-code}\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps):\n    \"\"\"\n    Predict next word using Greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n\n        # Returning iterations as df\n        return tokenizer.decode(input_ids[0])\n```\n:::\n\n\nLet us pass `i love` and predict next few word using our custom function and compare the result with `transformers` library.\n\n::: {#7fcd7751 .cell execution_count=3}\n``` {.python .cell-code}\ninput_text = \"i love\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\ngenerated_text = greedy_search(input_text=input_text,\n                              input_ids=input_ids,\n                              time_steps=20     # Generating Next 20 tokens\n                              )\nprint(generated_text)\n```\n:::\n\n\nThe output of our custom `greedy_search()` is\n\n```{plain text}\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\n```\n\nLet’s predicted tokens using `transformers` library.\n\n::: {#1088c25a .cell execution_count=4}\n``` {.python .cell-code}\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=20)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\n```\n:::\n\n\nOutput of `transformer` library is \n\n```{plain text}\ni love the idea of a \"new\" version of the game, but I'm not sure if it's\n```\n\nGenerated words are exactly same hah!!\n\nSince this is an deterministic approach predicted words are exactly same.\n\nIf you want to find out what are the other probable tokens and what is there probability you can try this code.\n\n::: {#6e988e47 .cell execution_count=5}\n``` {.python .cell-code}\n# Defining a function to finf the next word using greedy search\ndef greedy_search(input_text, input_ids, time_steps, choices_per_step):\n    \"\"\"\n    Predict next word using greedy Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n        time_steps (int): Number of next steps to predict\n        choices_per_step (int): Number of choice at each step.\n    \n    \"\"\"\n\n    iterations = []\n\n    with torch.no_grad():\n        for _ in range(time_steps):\n            iteration = dict()\n            iteration['input'] = tokenizer.decode(input_ids[0])\n            \n            # Predicting using model\n            output = model(input_ids = input_ids)\n            next_token_logits = output.logits[0,-1,:]\n            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n\n            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n\n            # Top few highest tokens\n            for choics_idx in range(choices_per_step):\n                token_index_sorted = sorted_index_of_next_probability[choics_idx]\n                token_prob = next_token_probability[token_index_sorted].cpu().numpy()\n                token_choice = f\"{tokenizer.decode(token_index_sorted)} ({token_prob*100:.2f}%)\"\n                iteration[f'Choice {choics_idx+1}'] = token_choice\n            \n            # Appending predicted next token to input\n            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n            iterations.append(iteration)\n\n        # Returning iterations as df\n        return pd.DataFrame(iterations)\n```\n:::\n\n\n## Beam Search\nOne of the main disadvantage of Greedy Search is, it fails to find out high probable words hidden behind low probability word. To fix this, there’s a smarter method called Beam Search. Beam Search doesn’t just pick one word at a time—it keeps track of a few possible paths and explores them to find the best combination of words. \n\nLet’s see how it works with an easy example:\n\n![fig-1: Beam Search Example](Screenshot 2025-04-19 231147.png)\n\nIn the preceding example, we've considered the initial tokens `i` and `love` The model predicts five potential next tokens, with the exhibiting `the` highest probability. Consequently, a Greedy search strategy would select `the` as the subsequent token, followed by the token with the highest probability given `the`, which in this case is `idea`.\n\nBut in Beam search for `2` beams we will be calculating conditional probability of next 2 tokens. The conditional probabilities are:\n\n$$\nP('the','way'|'i','love') = 0.25 \\times 0.03 = 0.0075\n$$\n$$\nP('the','idea'|'i','love')=0.25\\times 0.45 = 0.1125\n$$\n$$\nP('the','look'|'i','love')=0.25\\times0.25=0.0625\n$$\n$$...$$\n<p style=\"color: orange;\">\n$$\nP('it','.'|'i','love')=0.23\\times0.6=0.138\n$$\n</p>\n$$\nP('it','mom'|'i','love')=0.23\\times0.4=0.092\n$$\nWe can observe, the conditional probability of the sequence `it` and `.` is the highest among the considered pairs. Therefore, in Beam Search with a beam width of 2, `it` will be selected as the first subsequent token, rather than `the`, despite `the` having the highest individual probability in the previous step.\n\nLet’s try to write a function for beam search\n\n::: {#812d6945 .cell execution_count=6}\n``` {.python .cell-code}\n# Beam search function\ndef beam_search(input_text, max_length=50, num_beams=5):\n    \"\"\"\n    Predict next word using Beam Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        num_beams (int): Number of Beams to consider.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    # Initialize beams: Each beam starts with the same initial input\n    beams = [(input_ids, 0)]  # Tuple of (tokens, score)\n    \n    for _ in range(max_length):\n        new_beams = []\n        \n        for tokens, score in beams:\n            # Get model predictions\n            with torch.no_grad():\n                outputs = model(tokens)\n            \n            # Extract logits for the last token and apply softmax\n            logits = outputs.logits[:, -1, :]\n            probs = torch.softmax(logits, dim=-1)\n            \n            # Get top beam candidates\n            top_probs, top_indices = probs.topk(num_beams)\n            \n            # Create new beams\n            for i in range(num_beams):\n                new_token = top_indices[:, i].unsqueeze(-1)\n                new_score = score + torch.log(top_probs[:, i])  # Update score\n                \n                new_beam = (torch.cat([tokens, new_token], dim=-1), new_score)\n                new_beams.append(new_beam)\n        \n        # Sort beams by score and keep the best ones\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:num_beams]\n    \n    # Select the best final beam\n    best_tokens = beams[0][0]\n    return tokenizer.decode(best_tokens[0], skip_special_tokens=True)\n```\n:::\n\n\nLet's Try with the same input `i love` and then try to generate next 2 tokens using `beam_search()` function:\n```python\n# Example usage\ninput_text = \"i love\"\ngenerated_text = beam_search(input_text, max_length=10, num_beams=2)  # num_beams = 1 is same as greedy search\nprint(generated_text)\n```\n\n\n```{plain text}\ni love it. I love it. I love it.\n```\n\n\n*One thing to note, Beam Search with one beam is same as greedy search.(Verify!)*\n\nNow Let’s us compare the result form our function and output from `transformers` library.\n```python\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# Generating data from model using Hugging face library\noutput = model.generate(**input_tokens, max_new_tokens=10, num_beams = 3)\nprint(tokenizer.decode(output[0], skip_special_tokens= True))\n```\n\n\n```{plain text}\ni love it. I love it. I love it.\n```\n\nInterestingly, the outputs in both scenarios end up being identical. However, a noticeable issue is the repetitiveness of the generated text. This tendency towards repetition is a common drawback of deterministic decoding strategies. To mitigate this, we can introduce an element of randomness into the token selection process.\n\n## Top-k Search\n\nTo introduce randomness into the token selection process, one effective method is to randomly choose a token from the top *k* most probable predictions. This technique is known as **Top-k Sampling** or **Top-k Search**. The process involves the following steps:\n\n1. **Predict Probabilities:** First, the model predicts the probability distribution over all possible tokens.\n2. **Select Top-k:** We then identify and select the *k* tokens with the highest probabilities.\n3. **Normalize Probabilities:** The probabilities of these *k* selected tokens are re-normalized to create a new probability distribution.\n4. **Random Selection:** Finally, a token is randomly chosen from this renormalized distribution.\n\nLet's illustrate this with a Python function:\n```python\ndef top_k_search(input_text, max_length=50, k=5, show_option= False):\n\t\t\"\"\"\n    Predict next word using top k Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        k (int): Number of high probability token to consider.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      top_probs, top_indices = probs.topk(k)\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=1, k=5, show_option = True)  # num_beams = 1 is same as gready search\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n```\nTop 5 most probable tokens are:\n\n```{plain text}\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (22.54%)\n it (22.47%)\n you (21.72%)\n to (19.86%)\n this (13.4%)\n```\n\nThe next selected token is `the`. At this step, the five most probable predicted tokens are `the`, `it`, `you`, `to`, and `this`. From these five, one token will be randomly selected based on their respective probabilities.\n\nThe subsequent words were generated using the `transformers` library.\n\n```python\n# set seed to reproduce results. Feel free to change the seed though to get different results\nfrom transformers import set_seed\n\ninput_text = \"i love\"\n\n# Input Tokens\ninput_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n\n# set top_k to 50\nsample_output = model.generate(\n    **input_tokens,\n    max_new_tokens=4,\n    do_sample=True,\n    top_k=5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n```\n\n```{plain text}\ni love the \n```\n\nIn this instance, the predicted token is also `the`, although this is purely coincidental. Rerunning the code might yield a different token, but it's important to note that the subsequent selected token will invariably be one of the five most probable tokens from the preceding output (*as you can verify!*).\n\n```python\ninput_text = \"i love\"\ngenerated_text = top_k_search(input_text, max_length=10, k=5)\nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n```\n\n\n```{plain text}\n--------------------\nFinal generated Text is:\ni love the way he's doing this, but I'm\n--------------------\n```\n\n\nIf we predict next 10 tokens we can see there is not any repetition to tokens like previous case.\n\n## Top-p Search\n\nAnother effective method involves considering a dynamic set of the most probable tokens whose cumulative probability exceeds a predefined threshold, *p*. This technique is known as **Top-p Sampling** or **Nucleus Sampling**.\n\nThe process unfolds as follows:\n\n1. **Predict Probabilities:** Initially, the model predicts the probability distribution over all possible tokens.\n2. **Identify the Nucleus:** We then identify the smallest set of most probable tokens such that the sum of their probabilities is greater than or equal to the probability threshold *p*.\n3. **Normalize Probabilities:** The probabilities of the tokens within this selected set (the \"nucleus\") are renormalized to create a new probability distribution.\n4. **Random Selection:** Finally, a token is randomly chosen from this renormalized distribution.\n\nLet's illustrate this with a Python function:\n```python\ndef top_p_search(input_text, max_length=50, p=1, show_option= False):\n\t\t\"\"\"\n    Predict next word using top p Search.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = torch.softmax(logits, dim=-1)\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob<=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob<=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n\n# Example usage\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=1, p=0.5, show_option= True)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n```\n\n```{plain text}\nFor 1 Token:\n----------------------------------------------------------------------------------------------------\nSelected Token:  the\nTop K tokens are:\n\n the (17.06%)\n it (17.01%)\n you (16.44%)\n to (15.03%)\n this (10.14%)\n that (7.91%)\n my (5.68%)\n her (3.64%)\n him (3.56%)\n them (3.53%)\n--------------------\nFinal generated Text is:\ni love the\n--------------------\n```\n\nwith `p=0.5` when we predict next token there are `10` possible tokens. Out of then token `the` has been selected. You can rerun the code and verify weather your predicted token is out of these `10` tokens or not.\n\n## Temperature\n\nIntroducing randomness alone isn't always sufficient for generating desired text. When the goal is to extract and present information based on a specific document, prioritizing tokens with the highest probability is often preferred for accuracy and coherence. However, when crafting creative content like blog posts, encouraging the model to explore more diverse and unexpected word choices can lead to richer and more engaging outputs.\n\nThis balance between predictability and creativity can be effectively controlled using a parameter called **temperature**. The temperature value adjusts the probability distribution of the predicted tokens. A **low temperature** makes the distribution sharper, increasing the likelihood of selecting high-probability tokens and thus resulting in more focused and deterministic output. Conversely, a **high temperature** flattens the probability distribution, giving lower-probability tokens a greater chance of being selected, thereby injecting more randomness and creativity into the generated text.\n\nLet's define a custom `softmax` function that incorporates this `temperature` parameter:\n```python\nimport torch.nn.functional as F\n\n# Defining updated softmax for PyTorch tensors\ndef softmax_tensor(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n    \"\"\"\n    Applies the softmax function to a PyTorch tensor along the last dimension,\n    optionally with a temperature scaling factor.\n\n    Args:\n        logits: The input PyTorch tensor of logits.\n        temperature: A scaling factor for the logits (default: 1.0).\n\n    Returns:\n        A PyTorch tensor of the same shape as the input, with probabilities\n        along the last dimension.\n    \"\"\"\n    return F.softmax(logits / temperature, dim=-1)\n```\nUpdating Top-p Search function by including `temperature` parameter.\n\n```python\ndef top_p_search(prompt, max_length=50, p=1, temperature = 1, show_option = False):\n    \"\"\"\n    Predict next word using top p Search with temperature.\n\n    Args:\n        input_text (str): Input text Sequence.\n        max_length (int): Number of next token to predict.\n        p (float): Probability Threshold.\n        temperature (float): A number of control randomness.\n        show_option (bool): If true shows top k tokens at each step.\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input and move to device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # output_tokens\n    output_tokens = input_ids\n\n    for _ in range(max_length):\n      if show_option:\n        print(f\"\\nFor {_+1} Token:\")\n        print(\"-\"*100)\n\n      # Get model predictions\n      with torch.no_grad():\n          outputs = model(output_tokens)\n      \n      # Extract logits for the last token and apply softmax\n      logits = outputs.logits[:, -1, :]\n      probs = softmax_tensor(logits= logits, temperature= temperature + 1e-6)  # Added temperature while calculating probability\n      \n      # Get top beam candidates\n      probs, indices = torch.sort(probs, dim = 1, descending=True)\n      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n      top_probs = probs[:,:torch.sum(cumulative_prob<=p).item() + 1]\n      top_indices = indices[:,:torch.sum(cumulative_prob<=p).item() + 1]\n\n      # Normalizing top K probabilities\n      top_probs_norm = top_probs/torch.sum(top_probs)\n\n      # Choosing an element randomly based on normalized probability of top k tokens\n      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n      selected_token = top_indices[0][selected_token_id.item()]\n      \n      # Reshape selected_token to have shape (1, 1)\n      selected_token = selected_token.unsqueeze(0).unsqueeze(0) \n\n      # Appending selected  token with input token\n      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n      \n      if show_option:\n        # Printing Top K tokens\n        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n      \n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) \n\n# Example usage with very low temperature\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n```\nTokens generated with very low `temperature` of the model:\n\n```{plain text}\n--------------------\nFinal generated Text is:\ni love the idea of a \"super-hero\" who\n--------------------\n```\n\nConsequently, as you can observe, the initial few tokens closely mirror the results obtained with our earlier deterministic approach. Let's now explore the effect of increasing the `temperature`.\n\n```python\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=1)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n```\n\n```{plain text}\n--------------------\nFinal generated Text is:\ni love you no matter what,\" Trump wrote in a January\n--------------------\n```\n\n\nUpon increasing the `temperature` to `1`, we observe that the model begins to select more varied and less predictable words. However, it's important to note that employing excessively high `temperature` values can lead to generated text that lacks coherence and meaning. A generally recommended and effective range for the temperature parameter is between `0` and `1`.\n\n```python\ninput_text = \"i love\"\ngenerated_text = top_p_search(input_text, max_length=10, p=0.7, temperature=0.7)  \nprint(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)\n```\n\n\n```{plain text}\n--------------------\nFinal generated Text is:\ni love you so much.\n\nMORGAN:\n--------------------\n```\n\nText generated with `temperature` 0.7 feel more natural. (*Try playing with different `temperature` value*)\n\nYou can also see the list of all possible words by changing `show_option` parameter to `True` at different `temperature` value. ( Try playing with that as well, and see if you observe any pattern among the number of words and `temperature` value.)\n\nA special thank you to [Rohan-Paul-AI](https://www.youtube.com/@RohanPaul-AI) for the inspiration behind this post, and thanks also to [Koushik Khan](https://koushikkhan.github.io/) for encouraging me to write it!\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}